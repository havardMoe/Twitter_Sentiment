{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer\n",
    "from pyspark.sql.functions import col,isnan, when, count, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import emoji\n",
    "from preproc_functions import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|     default|\n",
      "|twitter_data|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('spark://10.10.28.172:7077') \\\n",
    "        .appName('pre_processing') \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(\"spark.pyfiles\", \"preproc_functions.py\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Preventing errors of having too many settings in hive-site.xml\n",
    "spark.sparkContext.setLogLevel('OFF')\n",
    "spark.sparkContext.addPyFile(\"preproc_functions.py\")\n",
    "# name of database\n",
    "spark.sql('use twitter_data')\n",
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing\n",
    "        1. Remove null rows (text and created_at and id) âœ…\n",
    "        2. Change emojis to words âœ…\n",
    "        3. Remove links âœ…\n",
    "        4. Remove unwanted charachters\n",
    "        5. Spell correction\n",
    "        6. Make list of words\n",
    "        7. Remove stop words (remove neutral words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Selecting all data from the raw_data table where text is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT *\n",
    "    FROM raw_data\n",
    "    WHERE text IS NOT NULL\n",
    "    AND created_at IS NOT NULL\n",
    "    AND id IS NOT NULL\n",
    "'''\n",
    "raw_data = spark.sql(query)\n",
    "# raw_data.count() # To validate that rows where removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Change Emojis to Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@TerryVernonsmi3 Trump's 2020 Campaign is Damaged Goods after cover-ups w ðŸ‡ºðŸ‡¦ Ukraine, and ðŸ‡·ðŸ‡º Russia, failed deals w ðŸ‡¨ðŸ‡³ China, ðŸ‡°ðŸ‡· N Korea and ðŸ‡®ðŸ‡· Iran, leaving KURDS to die, unhappy farmers, pissed off consumers, failed healthcare, extremely low approval ratings, his LIES and being labeled RACIST!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: OLA, need your opinion on thisss\n",
    "# 2.1 Change emojis to: emoji_meaning -> emoji meaning\n",
    "udf_emoji_to_words = udf(lambda text: snake_case_to_words(emoji_to_word(text)), StringType())\n",
    "raw_data.limit(5).collect()[4]['text'] # To see text from a tweet with emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"@TerryVernonsmi3 Trump's 2020 Campaign is Damaged Goods after cover-ups w  Ukraine  Ukraine, and  Russia  Russia, failed deals w  China  China,  south korea  N Korea and  Iran  Iran, leaving KURDS to die, unhappy farmers, pissed off consumers, failed healthcare, extremely low approval ratings, his LIES and being labeled RACIST!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.2 Change emojis to words\n",
    "raw_data = raw_data.withColumn('text', udf_emoji_to_words(col('text')))\n",
    "raw_data.limit(5).collect()[4]['text'] # Validation that emojis have been changed to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_remove_urls = udf(lambda text: remove_urls(text) , StringType())\n",
    "# TODO: Print text with URL(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.withColumn('text', udf_remove_urls(col('text')))\n",
    "# TODO: See the same line after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting only the interesting columns and create a new column where the text is tokenized and lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------------+----------+-------------------+--------------------+\n",
      "|                text|geo|                id| author_id|         created_at|              tokens|\n",
      "+--------------------+---+------------------+----------+-------------------+--------------------+\n",
      "|                text|geo|              null|      null|               null|              [text]|\n",
      "|EU hopes for posi...|   |572488029434200064|2745054115|2015-03-02 20:05:57|[eu, hopes, for, ...|\n",
      "|Ukraine pilot nea...|   |572488005732200448|2214245953|2015-03-02 20:05:51|[ukraine, pilot, ...|\n",
      "|EU Mediates Gas T...|   |572487964321820672|2396493517|2015-03-02 20:05:41|[eu, mediates, ga...|\n",
      "|Instead of sancti...|   |572487956570759168|  31025396|2015-03-02 20:05:39|[instead, of, san...|\n",
      "+--------------------+---+------------------+----------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = raw_data  #.select('id', 'text')\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='tokens')\n",
    "df = tokenizer.transform(df)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the tokens by the use of pyspark.ml.feature.StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------------+----------+-------------------+--------------------+--------------------+\n",
      "|                text|geo|                id| author_id|         created_at|              tokens|       filtered_text|\n",
      "+--------------------+---+------------------+----------+-------------------+--------------------+--------------------+\n",
      "|                text|geo|              null|      null|               null|              [text]|              [text]|\n",
      "|EU hopes for posi...|   |572488029434200064|2745054115|2015-03-02 20:05:57|[eu, hopes, for, ...|[eu, hopes, posit...|\n",
      "|Ukraine pilot nea...|   |572488005732200448|2214245953|2015-03-02 20:05:51|[ukraine, pilot, ...|[ukraine, pilot, ...|\n",
      "|EU Mediates Gas T...|   |572487964321820672|2396493517|2015-03-02 20:05:41|[eu, mediates, ga...|[eu, mediates, ga...|\n",
      "|Instead of sancti...|   |572487956570759168|  31025396|2015-03-02 20:05:39|[instead, of, san...|[instead, sanctio...|\n",
      "+--------------------+---+------------------+----------+-------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(),\n",
    "                       outputCol='filtered_text',\n",
    "                       caseSensitive=False)\n",
    "df = swr.transform(df)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: MORE PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: PREPARE FOR SENTIMENT ANALYSIS -> SAVE AS HIVE TABLE\n",
    "\n",
    "* RECREATE TO A PYTHON FILE SO THAT WE CAN RUN IT ON THE CLUSTER"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
