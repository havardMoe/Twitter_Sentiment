{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from analysis import WordList\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/27 20:27:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|   namespace|\n",
      "+------------+\n",
      "|     default|\n",
      "|twitter_data|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('spark://10.10.28.172:7077') \\\n",
    "        .appName('experimenting with runtimes') \\\n",
    "        .enableHiveSupport() \\\n",
    "        .config(\"spark.pyfiles\", \"analysis.py\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Preventing errors of having unused settings in hive-site.xml\n",
    "spark.sparkContext.setLogLevel('OFF')\n",
    "# path of dependency file(s)\n",
    "spark.sparkContext.addPyFile(\"/home/ubuntu/twitter_sentiment/code/analysis/analysis.py\")\n",
    "\n",
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+\n",
      "|    database|           tableName|isTemporary|\n",
      "+------------+--------------------+-----------+\n",
      "|twitter_data|  daily_sentiment_mr|      false|\n",
      "|twitter_data|      processed_data|      false|\n",
      "|twitter_data|            raw_data|      false|\n",
      "|twitter_data|             results|      false|\n",
      "|twitter_data|    textblob_results|      false|\n",
      "|twitter_data|       vader_results|      false|\n",
      "|twitter_data|wordlist2477_results|      false|\n",
      "+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('use twitter_data')\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score: -2\n"
     ]
    }
   ],
   "source": [
    "wl = WordList(r'2477')\n",
    "wordlist = wl.dict  # If you want to use the dictionary directly\n",
    "print('test score: ' + str(wl.analyze('hello this is a stupid test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation with MR\n",
    "# d = spark.sql('select to_date(created_at) as date, split(text, \" \") as word from processed_data')\n",
    "# rdd = d.rdd\n",
    "# rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# rdd.flatMapValues(lambda x: x) \\\n",
    "#     .mapValues(lambda x: wordlist[x]) \\\n",
    "#     .reduceByKey(add) \\\n",
    "#     .toDF(['date', 'total_sentiment']) \\\n",
    "#     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: timestamp, text: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udf_wl_analyze = udf(lambda text: wl.analyze(text), IntegerType())\n",
    "df = spark.sql('select created_at as date, text from processed_data')\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.withColumn('sentiment', udf_wl_analyze('text')). \\\n",
    "    groupBy(F.window('date', '1 day')). \\\n",
    "    sum(). \\\n",
    "    select([\n",
    "        F.to_date(F.col('window.start')).alias('date'), \n",
    "        F.col('sum(sentiment)').alias('total_sentiment')]). \\\n",
    "    write.mode('overwrite').saveAsTable('testing')\n",
    "    # select('d')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
